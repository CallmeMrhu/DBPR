1、MovieLen数据集中的数据，每个用户的打分时间过于集中，时间跨度不够大。
    如果使用timestamp进行划分会导致打分较早的用户在测试集中没有数据；
    而打分较迟的用户会在训练集中没有数据，出现冷启动问题。

2、使用Last-FM数据集，按照timestamp进行划分，不能按照之前处理MovieLen时直接取用户的前80%数据

3、在计算t时间段时，可以直接使用timestamp来进行划分
    其中划分方式可以采用pandas中的dataframe中对某一列进行过滤的方法，
        对下界timestamp加上时间间隔所转换成的timestamp值，则可以得到上界，
            于是就可以直接过滤得到所需要的数据，同理对各个时间段都可以采用相同的方法进行划分。

user_000001	2009-04-10T16:05:12Z	463a94f1-2713-40b1-9c88-dcc9c0170cae	Minus 8	f78c95a8-9256-4757-9a9f-213df5c6854e	Cold Fusion

user_000001                                 1
2009-04-10T16:05:12Z                        timestamp
f78c95a8-9256-4757-9a9f-213df5c6854e        traid

对时间序列的操作：
1、过滤掉user、item、timestamp为空的行
1、对过滤空行的数据集继续进行清理，其中将user_id用连续自然数表示，在music_id也用连续自然数表示，timestamp转换为用秒表示
2、先按照时间划分得到多个dataframe，当然不同的时间划分得到的dataframe的时间个数也不一样
3、第一个时间段t计算梯度时，上一个时间段的值可以用初始化的值表示，初始化为多少得思考
4、非第一个时间段t计算提督，需要用到上一个时间段的值时，则说明上一个时间段的Pu和Qi矩阵得单独保存一份。


t=0 419805
t=1 955470
t=2 1985952
t=3 2483596
t=4 2606951
t=5 2666297
sum 11118071
time interval : 1.5 years
validation/test : 1.5 years

Standard-BPR: step time
0 step :1411
1 step :1386
2 step :1496
3 step :1469
4 step :2639
5 step :1682
6 step :1378
7 step :1407
8 step :1413
9 step :1439
10 step :1449
11 step :1451
12 step :1423


DBPR: step time
0 step :2702
1 step :2871
2 step :4032
3 step :2941
4 step :2782
5 step :2855
6 step :5860

2018.1.12
根据之前讨论，接下来的任务是
1、更改时间间隔（interval）
2、使用别的度量标准 RMSE，可以使用已经训练好的矩阵来计算

2018.1.13
使用最好情况的userMat和itemMat来计算其他的度量标准
DBPR选择之前AUC最好情况时的userMat30.txt和itemMat30.txt
BPR选择之前AUC最好情况时的userMat48.txt和itemMat48.txt

2018.1.14
计算得到多种时间序列划分，然后分别计算得到AUC、Recall、Precision
2018.1.15
使用新的数据集进行训练、另外思考一下是否可以使用新的度量标准、以及对比实验还是不够完善

dataset:FineFoods
DPF复现

2018.1.20
1、对Epinions和FineFoods已经划分好的数据进行实验
2、对dPF进行复现，直接跑C++程序
3、计算recall和precision

2018.1.23
1、对dPF进行复现
2、继续寻找合适的数据集进行训练

dat -n 992 -m 983
data_Epinions -n 1461 -m 17765
data_FineFoods -n 1892 -m 19489
data_Netflix -n 23928 -m 17771

2018.4.14
在对数据进行划分时，使用drop([row])来删除不符合条件的行数会导致内存大大增加，几十个G的内存量，最后会被系统强制退出
最后是用来isin()的方法，即先对所有的睡觉进行判断，是否符合条件，符合为True，不符合为False，得到一个集合M
然后再使用df[M]来进行筛选，返回符合条件的行的dataframe。

对csv中浮点数转换为整数，即xxx.0的转换为xxx，遇到了一些麻烦，开始是想将数字转换为int，然后在转换为str型，发现执行起来特别慢；
于是直接对文件进行操作，读取文件，然后每行用line.strip('\n').split('\t')[0]/[1]...来进行获取，
对于xxx.0可以用(xxx.0).split('.')[0]来获取xxx部分，最后写入文件即可；且以下操作效果更好，既不会占用太多的内存，又可以提高执行速度
                    f = open(filepath)
                    while 1:
                        lines = f.readlines(10000)
                        if not line:
                            break
                        for line in lines:
                            #operation
2018.4.28
对几个数据集都训练完毕，但是git错误，误删除
对第一个时间段进行增加，来训练看是否有提高

2018.4.30
MovieLen
max_Timestamp: 1427784002
min_Timestamp 824835410
usernumber: 10702
itemnumber: 26231

movielen的validation和test中user的数量为1900左右
netflix的validation和test中user的数量为23420左右

main_running_standard.py 中 0.02-0.02-0.02的参数还没有训练

2018.5.1
目前来说MovieLen数据集在0.02-0.1-0.02和0.02-0.02-0.02上的结果还可以，但是AUC还未收敛，

2018.5.4
先写完论文，如果时间来得及再加一个timeSVD++

2018.5.7
对lastFM再次进行训练，求不去重的结果
对dPF进行训练，求不去重的结果，比较precision@10和precision@100

2018.5.8
对LastFM和MovieLen进行不去重复性计算，可以简化只计算其precision@10和precision@100
对dPF进行训练，求不去重的结果，比较precision@10和precision@100，其自己计算的结果比较高，但是用我们自己的precision来计算是很低的。
思考：DBPR如何像BPR一样使用概率进行解释...
思考：当前对user和item都做了dynamic embedding，但是对于音乐电影来说，item的动态性并不是那么强，所以是否可以试试static item embedding
Dynamic_BPR 最初的版本
Dynamic_BPR_auc.py 表示用auc作为衡量指标的
Dynamic_BPR_ndcg.py 表示用ndcg作为衡量指标的
Dynamic_BPR_static.py 表示user是dynamic representation，item为static
Dynamic_BPR_standard.py 表示最基本的BPR，user和item都为static

2018.5.9
1、对data_LastFM使用Dynamic_BPR_static.py训练的结果计算其他指标，似乎比Dynamic_BPR_auc.py要好
2、对netflix使用ndcg作为衡量指标训练得到的值计算其他指标，将dpf对netflix训练得到的ranking计算其他指标，比较DBPR是否会更好(结果还是不好,finish)
3、对LastFM进行static分析 evolution3 evolution6 evilution9 0.02-0.1-0.02 (finish)
4、求lastFM数据集的standard (running)
5、求FineFoods数据集的standard和evolution9，参数都参照之前调参的结果(finish)
6、对Netflix进行static分析 (finish)
7、对FineFoods进行static分析 evolution3 0.02-0.1-0.1(finish)
8、对MovieLen进行static分析 evolution9 0.02-0.02-0.02(finish)
等待数据集结果...完善论文实验部分

2018.5.10
1、对LastFM、FineFoods、MovieLen使用static模式参考ndcg进行训练(running)
2、试着对user保持static，item为dynamic，这样就做全来所有的user和item的dynamic的对比情况(finish)
3、对LastFM、FineFoods、MovieLen使用dynamic模式参考ndcg进行训练(running)
3、对Epinions数据集重新划分，然后重新计算dynamic和static (running)
epinions:
    max_Timestamp: 1060660800
    min_Timestamp 979102800
    usernumber: 22579
    itemnumber: 740029
等待staic和dynamic的数据结果，并进行对比，都是使用ndcg作为参考指标

2018.5.13

dynamic:
data_FineFoods: alpha_0.02_alphaReg_0.1_gama_0.0002 evolution3 计算结果
data_LastFM:    alpha_0.02_alphaReg_0.1_gama_0.0002 evolution9 计算结果
                alpha_0.02_alphaReg_0.1_gama_0.02 evolution0 evolution9 计算结果
data_MovieLen:  alpha_0.02_alphaReg_0.02_gama_0.0002 evolution9 计算结果

static:
data_FineFoods:alpha_0.02_alphaReg_0.1_gama_0.0002 evolution3 计算结果
data_LastFM:   alpha_0.02_alphaReg_0.1_gama_0.0002 evolution9 计算结果
data_MovieLen: alpha_0.02_alphaReg_0.02_gama_0.0002 evolution9 计算结果

对于时间序列的train、validation、test数据集的划分，理论上来讲应该没有validation数据集，直接用train和test
对于epinions数据集，item数目太大，导致最终的preciosn@10很小，可以取precison@100来计算。

2018.5.14
待做：对lastfm、movielens计算intervals为3、6的情况，记录并完善论文中size of the time intervals部分
待做：